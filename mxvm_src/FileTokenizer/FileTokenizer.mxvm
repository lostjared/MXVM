object ReadFile {
    section module { io, string }

    section data {
        string file_name, 256
        string file_mode     = "rb"
        export ptr f_buffer   = null
        export int  file_size = 0
        string prompt         = "Enter filename: "
        ptr    fptr          = null
        string file_err1     = "Could not open file: %s\n"
        int    nfile_size    = 0
        byte   zero_byte     = 0
    }

    section code {
        function readFile:
            # Prompt & read filename
            print prompt
            getline file_name
            # Strip trailing newline
            invoke strlen, file_name
            return nfile_size        # nfile_size = strlen
            sub nfile_size, 1
            load zero_byte, file_name, nfile_size, 1
            cmp zero_byte, 0xA
            jne skip_strip
            store zero_byte, file_name, nfile_size, 1
        skip_strip:

            # Open & size
            invoke fopen,  file_name, file_mode
            return fptr
            cmp fptr, 0
            je error_not_found

            invoke fsize, fptr
            return file_size

            # Allocate buffer (+1 for null)
            mov nfile_size, file_size
            add nfile_size, 1
            alloc f_buffer, 1, nfile_size

            # Read contents
        read_buffer:
            invoke fread, f_buffer, 1, file_size, fptr
            return nfile_size       # bytes_read into nfile_size
            store zero_byte, f_buffer, file_size, 1

            # Close
            invoke fclose, fptr
            ret

        error_not_found:
            invoke fprintf, stderr, file_err1, file_name
            exit 1
    }
}


object Tokenizer {
    section module { io, string }
    
    section data {
        export ptr t_buffer     = null   # filled by caller
        export int buf_len      = 0
        export ptr token_buf    = null
        export string output_fmt = "Token %d: %s\n"
        int i           = 0
        export int temp        = 0
        int len         = 0
        export int idx         = 0
        byte char_val   = 0
        byte space      = 32    # ASCII ' '
        int token_start = 0
        int token_num   = 0
        export byte zero       = 0
        export int count       = 0
    }

    section code {
        function init:
            alloc token_buf, 1, 256
            ret

        function tokenize:
            mov i,         0
            mov token_num, 0

        scan_loop:
            cmp i, buf_len
            jge done_tokenize
            load char_val, t_buffer, i, 1
            cmp char_val,  0
            je done_tokenize
            cmp char_val,  space
            jne found_nonspace
            add i, 1
            jmp scan_loop

        found_nonspace:
            mov token_start, i

        token_loop:
            load char_val, t_buffer, i, 1
            cmp char_val,  0
            je extract_token
            cmp char_val,  space
            je extract_token
            add i, 1
            jmp token_loop

        extract_token:
            mov len,        i
            sub len,        token_start
            mov temp,       0

        copy_char:
            cmp temp,       len
            jge after_copy
            mov idx,        token_start
            add idx,        temp
            load char_val,  t_buffer, idx, 1
            store char_val, token_buf, temp, 1
            add temp,1 
            jmp copy_char

        after_copy:
            store zero,        token_buf, temp, 1
            add token_num,1
            add count, 1
            print output_fmt, count, token_buf

            cmp char_val,   0
            je done_tokenize
            add i,1
            jmp scan_loop

        done_tokenize:
            ret

        function cleanup:
            free t_buffer
            free token_buf
            ret
    }
}


program FileTokenizer {
    section module { io, string }
    section object { ReadFile, Tokenizer }

    section data {
        int   pos        = 0
        int   line_start = 0
        byte  ch         = 0
        byte  newline    = 10   # ASCII '\n'
        byte  null_byte  = 0
    }

    section code {
        # Read the file into ReadFile.f_buffer, set ReadFile.file_size
        call ReadFile.readFile

        # Allocate tokenizerâ€™s working buffer (plus null)
        mov pos, ReadFile.file_size
        add pos, one
        alloc Tokenizer.t_buffer, 1, pos

        # Init token_buf
        call Tokenizer.init

        mov pos,        0
        mov line_start, 0

    read_loop:
        cmp pos, ReadFile.file_size
        jge process_last_line

        load ch, ReadFile.f_buffer, pos, 1
        cmp ch, newline
        jne cont_loop

        # Copy [line_start..pos) into Tokenizer.t_buffer
        mov Tokenizer.buf_len, pos
        sub Tokenizer.buf_len, line_start
        mov temp, 0

      copy_line:
        cmp temp, Tokenizer.buf_len
        jge after_line_copy
        mov idx,        line_start
        add idx,        temp
        load ch,        ReadFile.f_buffer, idx, 1    # <-- load from file buffer
        store ch,       Tokenizer.t_buffer, temp, 1
        add temp, 1
        jmp copy_line

      after_line_copy:
        store null_byte,Tokenizer.t_buffer, temp, 1
        call Tokenizer.tokenize
        mov line_start, pos
        add line_start, 1

    cont_loop:
        add pos, 1
        jmp read_loop

    process_last_line:
        cmp pos, line_start
        je icleanup

        mov Tokenizer.buf_len, pos
        sub Tokenizer.buf_len, line_start
        mov temp, 0

      copy_tail:
        cmp temp, Tokenizer.buf_len
        jge after_tail_copy
        mov idx,        line_start
        add idx,        temp
        load ch,        ReadFile.f_buffer, idx, 1
        store ch,       Tokenizer.t_buffer, temp, 1
        add temp,  1
        jmp copy_tail

      after_tail_copy:
        store zero,        Tokenizer.t_buffer, temp, 1
        call            Tokenizer.tokenize

    icleanup:
        call Tokenizer.cleanup
        free ReadFile.f_buffer
        done
    }
}
